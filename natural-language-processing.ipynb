{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aleksandrmorozov123/natural-language-processing?scriptVersionId=99551937\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","execution":{"iopub.status.busy":"2022-06-25T13:13:18.643132Z","iopub.execute_input":"2022-06-25T13:13:18.643403Z","iopub.status.idle":"2022-06-25T13:13:18.679362Z","shell.execute_reply.started":"2022-06-25T13:13:18.643376Z","shell.execute_reply":"2022-06-25T13:13:18.678687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Natural Language Processing - step by step**","metadata":{}},{"cell_type":"code","source":"# import required libraries\nimport numpy as np\nimport json\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2022-06-25T19:20:28.294018Z","iopub.execute_input":"2022-06-25T19:20:28.295312Z","iopub.status.idle":"2022-06-25T19:20:28.299748Z","shell.execute_reply.started":"2022-06-25T19:20:28.29527Z","shell.execute_reply":"2022-06-25T19:20:28.298696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read the data\ndf = pd.read_csv ('../input/twitter-sentiment-analysis-hatred-speech/test.csv')\ndf.head (5)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T19:20:30.653008Z","iopub.execute_input":"2022-06-25T19:20:30.653972Z","iopub.status.idle":"2022-06-25T19:20:30.73057Z","shell.execute_reply.started":"2022-06-25T19:20:30.653896Z","shell.execute_reply":"2022-06-25T19:20:30.729657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lowercase**","metadata":{}},{"cell_type":"code","source":"df['tweet'] = df['tweet'].apply (lambda x: \" \".join (x.lower () for x in x.split ()))\ndf['tweet']","metadata":{"execution":{"iopub.status.busy":"2022-06-25T19:20:32.691017Z","iopub.execute_input":"2022-06-25T19:20:32.691944Z","iopub.status.idle":"2022-06-25T19:20:32.77489Z","shell.execute_reply.started":"2022-06-25T19:20:32.691872Z","shell.execute_reply":"2022-06-25T19:20:32.773835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Remove punctuation**","metadata":{}},{"cell_type":"code","source":"df ['tweet'] = df['tweet'].str.replace (r\"\"\"[^\\w\\s]+\"\"\",\"\", regex = True)\n\ndf['tweet']","metadata":{"execution":{"iopub.status.busy":"2022-06-25T19:21:55.234714Z","iopub.execute_input":"2022-06-25T19:21:55.234991Z","iopub.status.idle":"2022-06-25T19:21:55.318246Z","shell.execute_reply.started":"2022-06-25T19:21:55.234963Z","shell.execute_reply":"2022-06-25T19:21:55.317371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Removing stop words**","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\n\n# remove stop words\nstop = stopwords.words ('english')\ndf['tweet'] = df['tweet'].apply (lambda x: \" \".join (x for x in x.split () if x not in stop))\ndf['tweet']","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:19.020148Z","iopub.execute_input":"2022-06-25T13:13:19.02061Z","iopub.status.idle":"2022-06-25T13:13:19.485556Z","shell.execute_reply.started":"2022-06-25T13:13:19.020567Z","shell.execute_reply":"2022-06-25T13:13:19.48471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Tokenizing step**","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\nTextBlob (df['tweet'][3]).words","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:19.487889Z","iopub.execute_input":"2022-06-25T13:13:19.488265Z","iopub.status.idle":"2022-06-25T13:13:19.495432Z","shell.execute_reply.started":"2022-06-25T13:13:19.488223Z","shell.execute_reply":"2022-06-25T13:13:19.494341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Stemming**","metadata":{}},{"cell_type":"code","source":"from nltk.stem import PorterStemmer\nst = PorterStemmer ()\ndf['tweet'][:5].apply (lambda x: \" \".join ([st.stem(word) for word in x.split ()]))","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:19.496571Z","iopub.execute_input":"2022-06-25T13:13:19.496768Z","iopub.status.idle":"2022-06-25T13:13:19.513564Z","shell.execute_reply.started":"2022-06-25T13:13:19.496744Z","shell.execute_reply":"2022-06-25T13:13:19.513015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lemmatizing**","metadata":{}},{"cell_type":"code","source":"from textblob import Word\n\ndf['tweet'] = df['tweet'].apply (lambda x: \" \".join ([Word(word).\n                                                     lemmatize () for word in x.split ()]))\n\ndf['tweet']","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:19.514481Z","iopub.execute_input":"2022-06-25T13:13:19.514808Z","iopub.status.idle":"2022-06-25T13:13:20.780306Z","shell.execute_reply.started":"2022-06-25T13:13:19.514775Z","shell.execute_reply":"2022-06-25T13:13:20.77934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Exploring text data**","metadata":{}},{"cell_type":"code","source":"# import required libraries\n\nimport nltk\nfrom nltk.corpus import webtext\nfrom nltk.probability import FreqDist\nfrom nltk.corpus import stopwords\nimport string\n\n# computer the frequency of all words\nfrequency_dist = nltk.FreqDist (df['tweet'][:30])\nfrequency_dist","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:20.781732Z","iopub.execute_input":"2022-06-25T13:13:20.782257Z","iopub.status.idle":"2022-06-25T13:13:20.789976Z","shell.execute_reply.started":"2022-06-25T13:13:20.782211Z","shell.execute_reply":"2022-06-25T13:13:20.789069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sorted_frequency_dist = sorted (frequency_dist, key = frequency_dist.__getitem__, reverse = True)\nsorted_frequency_dist","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:20.791111Z","iopub.execute_input":"2022-06-25T13:13:20.79134Z","iopub.status.idle":"2022-06-25T13:13:20.807159Z","shell.execute_reply.started":"2022-06-25T13:13:20.791313Z","shell.execute_reply":"2022-06-25T13:13:20.80615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Consider words with lengh greater than 5 and plot**","metadata":{}},{"cell_type":"code","source":"large_words = dict ([(k, v) for k, v in frequency_dist.items () if len (k) > 5])\nfrequency_dist = nltk.FreqDist (large_words)\nfrequency_dist.plot (50, cumulative = False)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:20.809446Z","iopub.execute_input":"2022-06-25T13:13:20.809799Z","iopub.status.idle":"2022-06-25T13:13:21.300853Z","shell.execute_reply.started":"2022-06-25T13:13:20.809766Z","shell.execute_reply":"2022-06-25T13:13:21.300043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Build wordcloud**","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud\ntcloud = WordCloud ().generate_from_frequencies (frequency_dist)\n\n# plotting the wordcloud\nimport matplotlib.pyplot as plt\nplt.imshow (tcloud, interpolation = 'bilinear')\nplt.axis (\"off\")\n(-0.5, 399.5, 199.5, -0.5)\nplt.show ()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:21.302205Z","iopub.execute_input":"2022-06-25T13:13:21.302646Z","iopub.status.idle":"2022-06-25T13:13:21.633466Z","shell.execute_reply.started":"2022-06-25T13:13:21.302614Z","shell.execute_reply":"2022-06-25T13:13:21.632559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Building a text preprocessing pipeline**","metadata":{}},{"cell_type":"code","source":"# read the data\ndatatweet = pd.read_csv ('../input/twitter-sentiment-analysis-hatred-speech/test.csv')\ndtweet = datatweet ['tweet'][:30]","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:21.635145Z","iopub.execute_input":"2022-06-25T13:13:21.635683Z","iopub.status.idle":"2022-06-25T13:13:21.680103Z","shell.execute_reply.started":"2022-06-25T13:13:21.635636Z","shell.execute_reply":"2022-06-25T13:13:21.679444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# write the function to process the tweets\ndef processRow (row):\n    import re\n    import nltk\n    from textblob import TextBlob\n    from nltk.corpus import stopwords\n    from nltk.stem import PorterStemmer\n    from textblob import Word\n    from nltk.util import ngrams\n    import re\n    from wordcloud import WordCloud, STOPWORDS\n    from nltk.tokenize import word_tokenize\n    \n    dtweet = row\n    \n    #lower case\n    dtweet = dtweet.apply (lambda x: \" \".join (x.lower () for x in x.split ()))\n    \n    #Removes unicode strings like \"\\u002c\" and \"x96\"\n    dtweet = dtweet.str.replace (r\"\"\"(\\\\u[0-9A-Fa-f]+)\"\"\",\"\")\n    dtweet = dtweet.str.replace (r\"\"\"[^\\x00-\\x7f]\"\"\",\"\")\n    \n    # convert any url to URL\n    dtweet = dtweet.str.replace(\"\"\"(www\\.[^\\s]+)|(htpps?://[^\\s]+)\"\"\", 'URL')\n    \n    # convert any @Username to \"AT_USER\"\n    dtweet = dtweet.str.replace (\"\"\"@[^\\s]+\"\"\", 'AT_USER')\n    \n    # remove additional white spaces\n    dtweet = dtweet.str.replace (\"\"\"[\\s]+\"\"\", ' ') \n    dtweet = dtweet.str.replace (\"\"\"[\\n]+\"\"\", ' ')\n    \n    # remove not alphanumeric symbols white spaces\n    dtweet = dtweet.str.replace (r\"\"\"[^\\w]\"\"\", ' ')\n    \n    # remove hashtag in front of a word \"\"\"\n    dtweet = dtweet.str.replace (r\"\"\"#([^\\s]+)\"\"\", r\"\"\"\\1\"\"\")\n    \n    # replace #word with word\n    dtweet = dtweet.str.replace (r\"\"\"#([^\\s]+)\"\"\", r\"\"\"\\1\"\"\")\n    \n    # remove :( or :)\n    dtweet = dtweet.replace (\"\"\":)\",\"\"\")\n    dtweet = dtweet.replace (\"\"\":(\",\"\"\")\n    \n    #remove numbers\n    dtweet = \" \".join([i for i in dtweet if not i.isdigit ()])\n    \n    # remove multiple exclamation\n    dtweet = re.sub (r\"\"\"(\\!)\\1+\"\"\", ' ', dtweet)\n    \n    # remove multiple question marks\n    dtweet = re.sub (r\"\"\"(\\?)\\1+\"\"\", ' ', dtweet)\n    \n    # remove multistop\n    dtweet = re.sub (r\"\"\"(\\.)\\1+\"\"\", ' ', dtweet)\n    \n    # lemma\n    from textblob import Word\n    dtweet = \" \".join ([Word(word).lemmatize () for word in dtweet.split ()])\n    \n    # stemmer\n    st = PorterStemmer ()\n    dtweet = \" \".join ([st.stem (word) for word in dtweet.split ()])\n    \n    # trim\n    dtweet = dtweet.strip ('\\'\"')\n    row = dtweet\n    return row\n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:21.681236Z","iopub.execute_input":"2022-06-25T13:13:21.681568Z","iopub.status.idle":"2022-06-25T13:13:21.69548Z","shell.execute_reply.started":"2022-06-25T13:13:21.681539Z","shell.execute_reply":"2022-06-25T13:13:21.694562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# call the function with data\nprocessRow (dtweet)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:21.696754Z","iopub.execute_input":"2022-06-25T13:13:21.696993Z","iopub.status.idle":"2022-06-25T13:13:21.743371Z","shell.execute_reply.started":"2022-06-25T13:13:21.696964Z","shell.execute_reply":"2022-06-25T13:13:21.742551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Converting text to features**\n- **One Hot encoding**","metadata":{}},{"cell_type":"code","source":"text = \"eat healthy live healthy everyday fix whats broken in 5 words eat healthy live healthy body mind soul spirit\"\n\npd.get_dummies (text.split ())","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:21.744549Z","iopub.execute_input":"2022-06-25T13:13:21.74478Z","iopub.status.idle":"2022-06-25T13:13:21.767105Z","shell.execute_reply.started":"2022-06-25T13:13:21.744752Z","shell.execute_reply":"2022-06-25T13:13:21.766213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Count vectorizing**","metadata":{}},{"cell_type":"code","source":"# import the function\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ntext = [\"user user user never understand dad left young deep in the feels\"]\n\n# create the transform\nvectorizer = CountVectorizer ()\n\n# tokenizing\nvectorizer.fit (text)\n\n# encode document\nvector = vectorizer.transform (text)\n\n# summarize and generating output\nprint (vectorizer.vocabulary_)\nprint (vector.toarray ())","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:21.768648Z","iopub.execute_input":"2022-06-25T13:13:21.769136Z","iopub.status.idle":"2022-06-25T13:13:21.782056Z","shell.execute_reply.started":"2022-06-25T13:13:21.769094Z","shell.execute_reply":"2022-06-25T13:13:21.781045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Generating N-grams using TextBlob**","metadata":{}},{"cell_type":"code","source":"text = 'haroldfriday have a weekend filled with sunbeams everyone healthy weekend'\n\n# import TextBlob\nfrom textblob import TextBlob\n\nTextBlob (text).ngrams (1)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:21.78371Z","iopub.execute_input":"2022-06-25T13:13:21.783962Z","iopub.status.idle":"2022-06-25T13:13:21.791225Z","shell.execute_reply.started":"2022-06-25T13:13:21.78393Z","shell.execute_reply":"2022-06-25T13:13:21.790373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TextBlob (text).ngrams (2)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:21.792414Z","iopub.execute_input":"2022-06-25T13:13:21.792911Z","iopub.status.idle":"2022-06-25T13:13:21.803339Z","shell.execute_reply.started":"2022-06-25T13:13:21.792883Z","shell.execute_reply":"2022-06-25T13:13:21.802376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Bigram-based features for a document**","metadata":{}},{"cell_type":"code","source":"# import the function\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ntext = ['enjoying the sunshine god is good orlando sunshinestate goodlife bosslady joy']\n\n# create the transform\nvectorizer.fit (text)\n\n# encode document\nvector = vectorizer.transform (text)\n\n# summarize and generating output\nprint (vectorizer.vocabulary_)\nprint (vector.toarray ())","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:21.804751Z","iopub.execute_input":"2022-06-25T13:13:21.805081Z","iopub.status.idle":"2022-06-25T13:13:21.818273Z","shell.execute_reply.started":"2022-06-25T13:13:21.805038Z","shell.execute_reply":"2022-06-25T13:13:21.81734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Co-occurence matrix**","metadata":{}},{"cell_type":"code","source":"# import required libraries\nimport numpy as np\nimport nltk\nfrom nltk import bigrams\nimport itertools\n\n# create function\n\ndef co_occurence_matrix (corpus):\n    vocab = set (corpus)\n    vocab = list (vocab)\n    vocab_to_index = {word:i for i, word in enumerate (vocab)}\n    # create bi-grams from all words in corpus\n    bi_grams = list (bigrams (corpus))\n    # frequency distribution of bi-grams\n    bigram_freq = nltk.FreqDist (bi_grams).most_common (len (bi_grams))\n    # Initialise co-occurence matrix\n    co_occurence_matrix = np.zeros ((len (vocab), len (vocab)))\n    \n    # loop through the bigrams taking thr current and previous words\n    for bigram in bigram_freq:\n        current = bigram [0][1]\n        previous = bigram [0][1]\n        count = bigram [1]\n        pos_current = vocab_to_index [current]\n        pos_previous = vocab_to_index [previous]\n        co_occurence_matrix [pos_current][pos_previous] = count\n    co_occurence_matrix = np.matrix (co_occurence_matrix)\n    # return the matrix and the index\n    return co_occurence_matrix, vocab_to_index","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:21.819858Z","iopub.execute_input":"2022-06-25T13:13:21.820307Z","iopub.status.idle":"2022-06-25T13:13:21.832212Z","shell.execute_reply.started":"2022-06-25T13:13:21.820271Z","shell.execute_reply":"2022-06-25T13:13:21.831454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences = [['user', 'never', 'understand', 'dad', 'left', 'young', 'deep', 'in', 'the', 'feels'],\n             ['enjoying', 'the', 'sunshine', 'god', 'is', 'good', 'orlando', 'sunshinestate', 'goodlife', 'bosslady', 'joy'],\n             ['haroldfriday', 'have', 'a', 'weekend', 'filled', 'with', 'sunbeams', 'everyone', 'healthy', 'weekend']]\n\n# create one list using many lists\nmerged = list (itertools.chain.from_iterable (sentences))\nmatrix = co_occurence_matrix (merged)\n\n# generate the matrix\nCoMatrixFinal = pd.DataFrame (matrix [0])\nprint (CoMatrixFinal)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:21.833549Z","iopub.execute_input":"2022-06-25T13:13:21.833999Z","iopub.status.idle":"2022-06-25T13:13:21.881831Z","shell.execute_reply.started":"2022-06-25T13:13:21.833948Z","shell.execute_reply":"2022-06-25T13:13:21.880978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Hash vectorizer**","metadata":{}},{"cell_type":"code","source":"# import required libraries\nfrom sklearn.feature_extraction.text import HashingVectorizer\n\n# list of text documents\ntext = ['and the forecast looks good for the weather all across bolton']\n\n# transform\nvectorizer = HashingVectorizer (n_features = 10)\n\n# create the hashing vector\nvector = vectorizer.transform (text)\n\n# summarize the vector\nprint (vector.shape)\nprint (vector.toarray ())","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:21.883119Z","iopub.execute_input":"2022-06-25T13:13:21.883911Z","iopub.status.idle":"2022-06-25T13:13:21.893152Z","shell.execute_reply.started":"2022-06-25T13:13:21.883868Z","shell.execute_reply":"2022-06-25T13:13:21.892472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Converting text to features using TF-IDF**","metadata":{}},{"cell_type":"code","source":"text = ['i have been working on my anatomy study guide since 5 pm and i am still not done isuck plspassme']\n\n# import TfidVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# create the transform\nvectorizer = TfidfVectorizer ()\n\n# tokenize and build vocab\nvectorizer.fit (text)\n\n# summarize\nprint (vectorizer.vocabulary_)\nprint (vectorizer.idf_)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:21.894618Z","iopub.execute_input":"2022-06-25T13:13:21.894836Z","iopub.status.idle":"2022-06-25T13:13:21.910648Z","shell.execute_reply.started":"2022-06-25T13:13:21.894811Z","shell.execute_reply":"2022-06-25T13:13:21.909924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Implementing word embeddings**","metadata":{}},{"cell_type":"code","source":"sentences = [['user', 'never', 'understand', 'dad', 'left', 'young', 'deep', 'in', 'the', 'feels'],\n             ['enjoying', 'the', 'sunshine', 'god', 'is', 'good', 'orlando', 'sunshinestate', 'goodlife', 'bosslady', 'joy'],\n             ['haroldfriday', 'have', 'a', 'weekend', 'filled', 'with', 'sunbeams', 'everyone', 'the', 'healthy', 'weekend'],\n            ['i', 'have', 'been', 'working', 'on', 'my', 'anatomy', 'study', 'guide', 'since', '5', 'pm', 'and', 'i', 'am', 'still', 'not', 'done', 'isuck', 'the', 'plspassme']]\n\n# import required libraries\nimport gensim\nfrom gensim.models import Word2Vec\nfrom sklearn.decomposition import PCA\nfrom matplotlib import pyplot\n\n\n# training the model\nskipgram = Word2Vec(sentences, window = 3, min_count=1,sg = 1)\n\nprint (skipgram)\n\n# visualize\nX = skipgram.wv.get_normed_vectors()\npca = PCA (n_components = 2)\nresult = pca.fit_transform (X)\n\n# create a scatter plot of the projection\npyplot.scatter (result [:, 0], result [:, 1])\nwords = len (skipgram.wv)\npyplot.show ()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:21.911612Z","iopub.execute_input":"2022-06-25T13:13:21.912134Z","iopub.status.idle":"2022-06-25T13:13:22.309688Z","shell.execute_reply.started":"2022-06-25T13:13:21.912091Z","shell.execute_reply":"2022-06-25T13:13:22.308888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Continuous bag of words (CBOW)**","metadata":{}},{"cell_type":"code","source":"# import required libraries\nfrom gensim.models import Word2Vec\nfrom sklearn.decomposition import PCA\nfrom matplotlib import pyplot\n\n# example sentences\nsentences = [['user', 'never', 'understand', 'dad', 'left', 'young', 'deep', 'in', 'the', 'feels'],\n             ['enjoying', 'the', 'sunshine', 'god', 'is', 'good', 'orlando', 'sunshinestate', 'goodlife', 'bosslady', 'joy'],\n             ['haroldfriday', 'have', 'a', 'weekend', 'filled', 'with', 'sunbeams', 'everyone', 'the', 'healthy', 'weekend'],\n            ['i', 'have', 'been', 'working', 'on', 'my', 'anatomy', 'study', 'guide', 'since', '5', 'pm', 'and', 'i', 'am', 'still', 'not', 'done', 'isuck', 'the', 'plspassme']]\n\n# training the model\ncbow = Word2Vec (sentences, vector_size = 100, window = 3, min_count = 1, sg = 1)\nprint (cbow)\n\n# save model\ncbow.save ('cbow.bin')\n\n# load model\ncbow = Word2Vec.load ('cbow.bin')\n\n# visualize\nX = cbow.wv.get_normed_vectors()\npca = PCA (n_components = 2)\nresult = pca.fit_transform (X)\n\n# create a scatter plot of the projection\npyplot.scatter (result [:, 0], result [:, 1])\nwords = len (cbow.wv)\npyplot.show ()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:22.316095Z","iopub.execute_input":"2022-06-25T13:13:22.316349Z","iopub.status.idle":"2022-06-25T13:13:22.550833Z","shell.execute_reply.started":"2022-06-25T13:13:22.316318Z","shell.execute_reply":"2022-06-25T13:13:22.55006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gensim\n# load the saved \nmodel = gensim.models.Word2Vec (dtweet)\n\n# checking how similarity works\nprint (model.wv.most_similar ('book', 'is'))","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:22.552322Z","iopub.execute_input":"2022-06-25T13:13:22.552739Z","iopub.status.idle":"2022-06-25T13:13:22.582082Z","shell.execute_reply.started":"2022-06-25T13:13:22.552704Z","shell.execute_reply":"2022-06-25T13:13:22.581406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Implementing fastText**","metadata":{}},{"cell_type":"code","source":"# import FastText\nfrom gensim.models import FastText\nfrom sklearn.decomposition import PCA\nfrom matplotlib import pyplot\n\n# Example sentences\nsentences = [['user', 'never', 'understand', 'dad', 'left', 'young', 'deep', 'in', 'the', 'feels'],\n             ['enjoying', 'the', 'sunshine', 'god', 'is', 'good', 'orlando', 'sunshinestate', 'goodlife', 'bosslady', 'joy'],\n             ['haroldfriday', 'have', 'a', 'weekend', 'filled', 'with', 'sunbeams', 'everyone', 'the', 'healthy', 'weekend'],\n            ['i', 'have', 'been', 'working', 'on', 'my', 'anatomy', 'study', 'guide', 'since', '5', 'pm', 'and', 'i', 'am', 'still', 'not', 'done', 'isuck', 'the', 'plspassme']]\nfast = FastText (sentences, vector_size = 20, window = 1, min_count = 1, workers = 5, min_n = 1, max_n = 2)\n\n# vector for word the\nprint (fast)\n\n# visualize\nX = fast.wv.get_normed_vectors()\npca = PCA (n_components = 2)\nresult = pca.fit_transform (X)\n\n# create a scatter plot of the projection\npyplot.scatter (result [:, 0], result [:, 1])\nwords = len (fast.wv)\npyplot.show ()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:22.583207Z","iopub.execute_input":"2022-06-25T13:13:22.583726Z","iopub.status.idle":"2022-06-25T13:13:23.023709Z","shell.execute_reply.started":"2022-06-25T13:13:22.583691Z","shell.execute_reply":"2022-06-25T13:13:23.022811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Extracting noun phrases**","metadata":{}},{"cell_type":"code","source":"# import required libraries\nimport nltk\nfrom textblob import TextBlob\n\n# extract noun\nblob = TextBlob ('i have been working on my anatomy study guide since 5 pm and i am still not done isuck plspassme')\nfor np in blob.noun_phrases:\n    print (np)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:23.024743Z","iopub.execute_input":"2022-06-25T13:13:23.024956Z","iopub.status.idle":"2022-06-25T13:13:27.756531Z","shell.execute_reply.started":"2022-06-25T13:13:23.02493Z","shell.execute_reply":"2022-06-25T13:13:27.755577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Find the similarity**","metadata":{}},{"cell_type":"code","source":"documents = ('eat healthy live healthy everyday fix whats broken in 5 words eat healthy live healthy body mind soul spirit', \n             'and the forecast looks good for the weather all across bolton',\n            '3rd bihday amazing hilarious nephew eli ahmir uncle dave love misses')\n\n# import libraries\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# compute tfidf\ntfidf_vectorizer = TfidfVectorizer ()\ntfidf_matrix = tfidf_vectorizer.fit_transform (documents)\ntfidf_matrix.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:27.757749Z","iopub.execute_input":"2022-06-25T13:13:27.757956Z","iopub.status.idle":"2022-06-25T13:13:27.768891Z","shell.execute_reply.started":"2022-06-25T13:13:27.75793Z","shell.execute_reply":"2022-06-25T13:13:27.767893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compute similarity for first sentence with rest of the sentences\ncosine_similarity (tfidf_matrix [0:2], tfidf_matrix)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:27.770098Z","iopub.execute_input":"2022-06-25T13:13:27.770304Z","iopub.status.idle":"2022-06-25T13:13:27.784478Z","shell.execute_reply.started":"2022-06-25T13:13:27.770276Z","shell.execute_reply":"2022-06-25T13:13:27.783524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Tagging part of speech**","metadata":{}},{"cell_type":"code","source":"text = 'one of the worlds greatest spoing events lemans24 team audi'\n\n# import required libraries\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nstop_words = set (stopwords.words ('english'))\n\n# tokenize the text\ntokens = sent_tokenize (text)\n\n# generate tagging for all tokens using loop\nfor i in tokens:\n    words = nltk.word_tokenize (i)\n    words = [w for w in words if not w in stop_words]\n    # POS-tagger\n    tags = nltk.pos_tag (words)\ntags","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:27.785615Z","iopub.execute_input":"2022-06-25T13:13:27.785853Z","iopub.status.idle":"2022-06-25T13:13:27.942738Z","shell.execute_reply.started":"2022-06-25T13:13:27.785825Z","shell.execute_reply":"2022-06-25T13:13:27.941768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Using SpaCy**","metadata":{}},{"cell_type":"code","source":"import spacy\nnlp = spacy.load ('en_core_web_sm')\n\n# create a sentence\ndoc = nlp(u'one of the worlds greatest sporting events lemans24 team audi')\n\nfor ent in doc.ents:\n    print (ent.text, ent.start_char, ent.end_char, ent.label_)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:27.94397Z","iopub.execute_input":"2022-06-25T13:13:27.94421Z","iopub.status.idle":"2022-06-25T13:13:31.652455Z","shell.execute_reply.started":"2022-06-25T13:13:27.944184Z","shell.execute_reply":"2022-06-25T13:13:31.651505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Extracting topics from text**","metadata":{}},{"cell_type":"code","source":"doc1 = \"lipo-light helped shape her, and it can help shape you. learn more@user loseinches burnfat result\"\ndoc2 = \"one of the worlds greatest sporting events lemans24 team audi\"\ndoc3 = \"and the forecast looks good for the weather all across bolton\"\n\ndoc_complete = [doc1, doc2, doc3]\ndoc_complete","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:31.653925Z","iopub.execute_input":"2022-06-25T13:13:31.654182Z","iopub.status.idle":"2022-06-25T13:13:31.660238Z","shell.execute_reply.started":"2022-06-25T13:13:31.654152Z","shell.execute_reply":"2022-06-25T13:13:31.659611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import required libraries\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport string\n\n# text preprocessing\nstop = set (stopwords.words ('english'))\nexclude = set (string.punctuation)\nlemma = WordNetLemmatizer ()\ndef clean (doc):\n    stop_free = \" \".join ([i for i in doc.lower ().split () if i not in stop])\n    punc_free = \" \".join (ch for ch in stop_free if ch not in exclude)\n    normalized = \" \".join (lemma.lemmatize (word) for word in punc_free.split ())\n    return normalized\n\ndoc_clean = [clean (doc).split () for doc in doc_complete]\ndoc_clean","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:31.661454Z","iopub.execute_input":"2022-06-25T13:13:31.662001Z","iopub.status.idle":"2022-06-25T13:13:31.680558Z","shell.execute_reply.started":"2022-06-25T13:13:31.661965Z","shell.execute_reply":"2022-06-25T13:13:31.679943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Preparing document term matrix**","metadata":{}},{"cell_type":"code","source":"# importing gensim\nimport gensim\nfrom gensim import corpora\n\n# creating the term dictionary of our corpus, where every unique term is assigned an index\ndictionary = corpora.Dictionary (doc_clean)\n\n# converting a list of documents (corpus) into Document-Term matrix using dictionary prepared above\ndoc_term_matrix = [dictionary.doc2bow (doc) for doc in doc_clean]\n\ndoc_term_matrix","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:31.681825Z","iopub.execute_input":"2022-06-25T13:13:31.682553Z","iopub.status.idle":"2022-06-25T13:13:31.69409Z","shell.execute_reply.started":"2022-06-25T13:13:31.682518Z","shell.execute_reply":"2022-06-25T13:13:31.693407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**LDA model**","metadata":{}},{"cell_type":"code","source":"# creating the object for LDA model using gensim library\nLda = gensim.models.ldamodel.LdaModel\n\n# runnug and trainig LDA model on the document term matrix for 3 topics\nldamodel = Lda (doc_term_matrix, num_topics = 3, id2word = dictionary, passes = 50)\n\n# results\nprint (ldamodel.print_topics (num_topics = 20, num_words = 20))","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:31.695383Z","iopub.execute_input":"2022-06-25T13:13:31.696206Z","iopub.status.idle":"2022-06-25T13:13:31.831888Z","shell.execute_reply.started":"2022-06-25T13:13:31.696163Z","shell.execute_reply":"2022-06-25T13:13:31.831041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Classifying text**","metadata":{}},{"cell_type":"code","source":"# read the data \nemail = pd.read_csv ('../input/email-spam-dataset/enronSpamSubset.csv') \n\n# understanding data\nemail.columns","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:31.83311Z","iopub.execute_input":"2022-06-25T13:13:31.833351Z","iopub.status.idle":"2022-06-25T13:13:32.152786Z","shell.execute_reply.started":"2022-06-25T13:13:31.833322Z","shell.execute_reply":"2022-06-25T13:13:32.151926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"email = email.rename (columns = {\"Body\": \"Email\"})\nemail.head (10)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:32.154088Z","iopub.execute_input":"2022-06-25T13:13:32.154967Z","iopub.status.idle":"2022-06-25T13:13:32.168151Z","shell.execute_reply.started":"2022-06-25T13:13:32.154921Z","shell.execute_reply":"2022-06-25T13:13:32.167091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Text preprocessing and feature engineering**","metadata":{}},{"cell_type":"code","source":"# import required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport string\nfrom nltk.stem import SnowballStemmer\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom textblob import TextBlob\nfrom nltk.stem import PorterStemmer\nfrom textblob import Word\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n\n# preprocessing steps like lower case, stemming and lemmatization\nemail [\"Email\"] = email [\"Email\"].apply (lambda x: \" \".join (x.lower () for x in x.split ()))\nstop = stopwords.words ('english')\nemail [\"Email\"] = email [\"Email\"].apply (lambda x: \" \".join (x for x in x.split () if x not in stop))\nst = PorterStemmer ()\nemail [\"Email\"] = email [\"Email\"].apply (lambda x: \" \".join ([st.stem (word) for word in x.split ()]))\nemail [\"Email\"] = email [\"Email\"].apply (lambda x: \" \".join ([Word (word).lemmatize () for word in x.split ()]))\nemail.head (10)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:13:32.169585Z","iopub.execute_input":"2022-06-25T13:13:32.16988Z","iopub.status.idle":"2022-06-25T13:14:40.681793Z","shell.execute_reply.started":"2022-06-25T13:13:32.169838Z","shell.execute_reply":"2022-06-25T13:14:40.680935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# splitting data into train and validation\ntrain_x, valid_x, train_y, valid_y = train_test_split (email [\"Email\"], email [\"Label\"])\n\n# TFIDF feature generation for a maximum of 6000 features\nencoder = preprocessing.LabelEncoder ()\ntrain_y = encoder.fit_transform (train_y)\nvalid_y = encoder.fit_transform (valid_y)\n\ntfidf_vect = TfidfVectorizer (analyzer = 'word', token_pattern = r'\\w{1,}', max_features = 6000)\ntfidf_vect.fit (email [\"Email\"])\nxtrain_tfidf = tfidf_vect.transform (train_x)\nxvalid_tfidf = tfidf_vect.transform (valid_x)\n\nxtrain_tfidf.data","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:40.68278Z","iopub.execute_input":"2022-06-25T13:14:40.682988Z","iopub.status.idle":"2022-06-25T13:14:44.273179Z","shell.execute_reply.started":"2022-06-25T13:14:40.682963Z","shell.execute_reply":"2022-06-25T13:14:44.272534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model training**","metadata":{}},{"cell_type":"code","source":"def train_model (classifier, feature_vector_train, label, feature_vector_valid, is_neural_net = False):\n    # fit the trainig dataset on the classifier\n    classifier.fit (feature_vector_train, label)\n    # predict the labels on validation dataset\n    predictions = classifier.predict (feature_vector_valid)\n    return metrics.accuracy_score (predictions, valid_y)\n\n# Naive Bayes training\naccuracy = train_model (naive_bayes.MultinomialNB (alpha = 0.2), xtrain_tfidf, train_y, xvalid_tfidf)\nprint (\"Accuracy: \", accuracy)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:44.27429Z","iopub.execute_input":"2022-06-25T13:14:44.274803Z","iopub.status.idle":"2022-06-25T13:14:44.291116Z","shell.execute_reply.started":"2022-06-25T13:14:44.274769Z","shell.execute_reply":"2022-06-25T13:14:44.289731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Linear classifier on Word level TF IDF Vectors\naccuracy = train_model (linear_model.LogisticRegression (), xtrain_tfidf, train_y, xvalid_tfidf)\nprint (\"Accuracy: \", accuracy)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:44.292126Z","iopub.execute_input":"2022-06-25T13:14:44.292758Z","iopub.status.idle":"2022-06-25T13:14:44.471708Z","shell.execute_reply.started":"2022-06-25T13:14:44.292717Z","shell.execute_reply":"2022-06-25T13:14:44.470811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Sentiment analysis**","metadata":{}},{"cell_type":"code","source":"# read the sample\nsentiment1 = \"it's unbelievable that in the 21st century we'd need something like this. again. neverump xenopho...\"\nsentiment2 = \"thank you! super love it! zpamdelacruz wedding dolores, capas tarlac\"\n\n# import required libraries\nfrom textblob import TextBlob\n\n# TextBlob has a pretrained sentiment prediction model\nblob = TextBlob (sentiment1)\nblob.sentiment","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:44.472976Z","iopub.execute_input":"2022-06-25T13:14:44.473205Z","iopub.status.idle":"2022-06-25T13:14:44.539061Z","shell.execute_reply.started":"2022-06-25T13:14:44.473178Z","shell.execute_reply":"2022-06-25T13:14:44.538232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now lets look at the sentiment2\nblob = TextBlob (sentiment2)\nblob.sentiment","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:44.539932Z","iopub.execute_input":"2022-06-25T13:14:44.540193Z","iopub.status.idle":"2022-06-25T13:14:44.54618Z","shell.execute_reply.started":"2022-06-25T13:14:44.540166Z","shell.execute_reply":"2022-06-25T13:14:44.545101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Multiclass classification**","metadata":{}},{"cell_type":"code","source":"# import required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport string\nfrom nltk.stem import SnowballStemmer\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom textblob import Word\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nimport sklearn.feature_extraction.text as text\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import cross_val_score\nfrom io import StringIO\nimport seaborn as sns\n\n# import data\ndata = pd.read_csv ('../input/comcastcomplaints/comcast_fcc_complaints_2015.csv')\ndata.head (10)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:44.547339Z","iopub.execute_input":"2022-06-25T13:14:44.547551Z","iopub.status.idle":"2022-06-25T13:14:44.927625Z","shell.execute_reply.started":"2022-06-25T13:14:44.547527Z","shell.execute_reply":"2022-06-25T13:14:44.926628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# selecting required columns and rows\ndata = data [['Description', 'Status']]\ndata = data [pd.notnull (data ['Description'])]\n\ndata.head (10)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:44.929264Z","iopub.execute_input":"2022-06-25T13:14:44.92963Z","iopub.status.idle":"2022-06-25T13:14:44.94464Z","shell.execute_reply.started":"2022-06-25T13:14:44.929586Z","shell.execute_reply":"2022-06-25T13:14:44.943786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# factorizing the rating column\ndata ['category_id'] = data['Status'].factorize ()[0]\ndata.head (10)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:44.945993Z","iopub.execute_input":"2022-06-25T13:14:44.946871Z","iopub.status.idle":"2022-06-25T13:14:44.959832Z","shell.execute_reply.started":"2022-06-25T13:14:44.946826Z","shell.execute_reply":"2022-06-25T13:14:44.959072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the distribution of complaints by category\ndata.groupby ('Status').Description.count ()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:44.961371Z","iopub.execute_input":"2022-06-25T13:14:44.961842Z","iopub.status.idle":"2022-06-25T13:14:44.970037Z","shell.execute_reply.started":"2022-06-25T13:14:44.961799Z","shell.execute_reply":"2022-06-25T13:14:44.969237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize it\nfig = plt.figure (figsize = (8, 6))\ndata.groupby ('Status').Description.count ().plot.bar (ylim = 0)\nplt.show ()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:44.971355Z","iopub.execute_input":"2022-06-25T13:14:44.971726Z","iopub.status.idle":"2022-06-25T13:14:45.171275Z","shell.execute_reply.started":"2022-06-25T13:14:44.971697Z","shell.execute_reply":"2022-06-25T13:14:45.17067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Splitting the data**","metadata":{}},{"cell_type":"code","source":"train_x, valid_x, train_y, valid_y = train_test_split (data ['Description'], data ['Status'])","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:45.172349Z","iopub.execute_input":"2022-06-25T13:14:45.172661Z","iopub.status.idle":"2022-06-25T13:14:45.177483Z","shell.execute_reply.started":"2022-06-25T13:14:45.172634Z","shell.execute_reply":"2022-06-25T13:14:45.176887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feature engineering using TF-IDF**","metadata":{}},{"cell_type":"code","source":"encoder = preprocessing.LabelEncoder ()\ntrain_y = encoder.fit_transform (train_y)\nvalid_y = encoder.fit_transform (valid_y)\n\ntfidf_vect = TfidfVectorizer (analyzer = 'word', token_pattern = r'\\w{1,}', max_features = 5000)\ntfidf_vect.fit (data['Description'])\nxtrain_tfidf = tfidf_vect.transform (train_x)\nxvalid_tfidf = tfidf_vect.transform (valid_x)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:45.178684Z","iopub.execute_input":"2022-06-25T13:14:45.178908Z","iopub.status.idle":"2022-06-25T13:14:46.571826Z","shell.execute_reply.started":"2022-06-25T13:14:45.178872Z","shell.execute_reply":"2022-06-25T13:14:46.570916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model building and evaluation**","metadata":{}},{"cell_type":"code","source":"model = linear_model.LogisticRegression().fit (xtrain_tfidf, train_y)\n\n# model summary\nLogisticRegression (C = 1.0, class_weight = None, dual = False, fit_intercept = True,\n                    intercept_scaling = 1, max_iter = 100, multi_class = \"ovr\", n_jobs = 1,\n                    penalty = 'l2', random_state = None, solver = 'liblinear', tol = 0.0001,\n                    verbose = 0, warm_start = False)\n\n# checking accuracy\naccuracy = metrics.accuracy_score (model.predict (xvalid_tfidf), valid_y)\nprint (\"Accuracy: \", accuracy)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:46.573062Z","iopub.execute_input":"2022-06-25T13:14:46.573295Z","iopub.status.idle":"2022-06-25T13:14:47.951639Z","shell.execute_reply.started":"2022-06-25T13:14:46.573265Z","shell.execute_reply":"2022-06-25T13:14:47.950735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# classification report\nprint (metrics.classification_report (valid_y, model.predict (xvalid_tfidf),\n                                     target_names = data ['Status'].unique ()))","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:47.95318Z","iopub.execute_input":"2022-06-25T13:14:47.954716Z","iopub.status.idle":"2022-06-25T13:14:47.984125Z","shell.execute_reply.started":"2022-06-25T13:14:47.954665Z","shell.execute_reply":"2022-06-25T13:14:47.982945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\n# confusion matrix\nconf_matrix = confusion_matrix (valid_y, model.predict (xvalid_tfidf))\n\n# visualizing confusion matrix\ncategory_id_df = data [['Status', \"category_id\"]].drop_duplicates ().sort_values ('category_id')\ncategory_to_id = dict (category_id_df.values)\nid_to_category = dict (category_id_df [['category_id', 'Status']].values)\n\nfig, ax = plt.subplots (figsize = (8, 6))\nsns.heatmap (conf_matrix, annot = True, fmt = 'd', cmap = \"BuPu\",\n            xticklabels = category_id_df [['Status']].values,\n            yticklabels = category_id_df [[\"Status\"]].values)\nplt.ylabel (\"Actual\")\nplt.xlabel (\"Predicted\")\nplt.show ()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:47.986373Z","iopub.execute_input":"2022-06-25T13:14:47.987815Z","iopub.status.idle":"2022-06-25T13:14:48.329842Z","shell.execute_reply.started":"2022-06-25T13:14:47.987767Z","shell.execute_reply":"2022-06-25T13:14:48.328887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prediction example\ntext = ['Comcast refuses to help troubleshoot and correct my service.']\ntext_features = tfidf_vect.transform (text)\npredictions = model.predict (text_features)\nprint (text)\nprint (\"  - Predicted as: '{}'\".format (id_to_category [predictions [0]]))","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:48.331763Z","iopub.execute_input":"2022-06-25T13:14:48.3321Z","iopub.status.idle":"2022-06-25T13:14:48.340222Z","shell.execute_reply.started":"2022-06-25T13:14:48.332057Z","shell.execute_reply":"2022-06-25T13:14:48.339293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Implementing sentiment analysis**","metadata":{}},{"cell_type":"code","source":"# import necesserary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# read the data\ndata = pd.read_csv ('../input/22000-scotch-whisky-reviews/scotch_review.csv')\n\n# Look at the top 10 rows of the data\ndata.head (10)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:48.341845Z","iopub.execute_input":"2022-06-25T13:14:48.342182Z","iopub.status.idle":"2022-06-25T13:14:48.398992Z","shell.execute_reply.started":"2022-06-25T13:14:48.342141Z","shell.execute_reply":"2022-06-25T13:14:48.398054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# understand the data types of the columns\ndata.info ()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:48.400301Z","iopub.execute_input":"2022-06-25T13:14:48.400564Z","iopub.status.idle":"2022-06-25T13:14:48.419041Z","shell.execute_reply.started":"2022-06-25T13:14:48.400533Z","shell.execute_reply":"2022-06-25T13:14:48.417985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# looking at the summary of descriptions\ndata.description.head (10)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:48.420242Z","iopub.execute_input":"2022-06-25T13:14:48.420499Z","iopub.status.idle":"2022-06-25T13:14:48.432461Z","shell.execute_reply.started":"2022-06-25T13:14:48.420468Z","shell.execute_reply":"2022-06-25T13:14:48.431589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Text preprocessing**","metadata":{}},{"cell_type":"code","source":"# import libraries\nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob\nfrom textblob import Word\n\n# lower casing and removing pucntuations\ndata ['description'] = data ['description'].apply (lambda x: \" \".join (x.lower () for x in x.split ()))\ndata ['description'] = data ['description'].str.replace (\"\"\"[^\\w\\s]\"\"\",\"\")\ndata.description.head (10)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:48.433412Z","iopub.execute_input":"2022-06-25T13:14:48.43364Z","iopub.status.idle":"2022-06-25T13:14:48.542086Z","shell.execute_reply.started":"2022-06-25T13:14:48.433611Z","shell.execute_reply":"2022-06-25T13:14:48.541129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove of stop words\nstop = stopwords.words ('english')\ndata ['description'] = data ['description'].apply (lambda x: \" \".join (x for x in x.split () if x not in stop))\ndata.description.head (10)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:48.543374Z","iopub.execute_input":"2022-06-25T13:14:48.543615Z","iopub.status.idle":"2022-06-25T13:14:48.855178Z","shell.execute_reply.started":"2022-06-25T13:14:48.543587Z","shell.execute_reply":"2022-06-25T13:14:48.854228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lemmatization\ndata ['description'] = data ['description'].apply (lambda x: \" \".join ([Word (word).\n                                                                       lemmatize () for word in x.split ()]))\ndata.description.head (10)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:48.856398Z","iopub.execute_input":"2022-06-25T13:14:48.856623Z","iopub.status.idle":"2022-06-25T13:14:49.685047Z","shell.execute_reply.started":"2022-06-25T13:14:48.856597Z","shell.execute_reply":"2022-06-25T13:14:49.684175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Exploratory data analysis**","metadata":{}},{"cell_type":"code","source":"# dropping null values\ndata.dropna (inplace = True)\n\n# the histogram reveals this dataset is highly unbalanced toward rating 85-90\ndata ['review.point'].hist (bins = 5, grid = False)\nplt.show ()\nprint (data.groupby (data['review.point']).count ())","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:49.686302Z","iopub.execute_input":"2022-06-25T13:14:49.686513Z","iopub.status.idle":"2022-06-25T13:14:49.885311Z","shell.execute_reply.started":"2022-06-25T13:14:49.686487Z","shell.execute_reply":"2022-06-25T13:14:49.884422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to make it balanced data, I sampled each score by the lowest n-count\nscore_1 = data [data ['review.point'] == 83].sample (n = 141)\nscore_2 = data [data ['review.point'] == 85].sample (n = 198)\nscore_3 = data [data ['review.point'] == 87].sample (n = 221)\nscore_4 = data [data ['review.point'] == 90].sample (n = 188)\nscore_5 = data [data ['review.point'] == 93].sample (n = 84)\n\n# here I create a balanced dataset\nreviews_sample = pd.concat ([score_1, score_2, score_3, score_4, score_5], axis = 0)\nreviews_sample.reset_index (drop = True, inplace = True)\n\n# printing count by 'review.point' to check dataset is now balanced\nprint (reviews_sample.groupby ('review.point').count ())","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:49.886809Z","iopub.execute_input":"2022-06-25T13:14:49.887228Z","iopub.status.idle":"2022-06-25T13:14:49.910305Z","shell.execute_reply.started":"2022-06-25T13:14:49.887192Z","shell.execute_reply":"2022-06-25T13:14:49.909279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's build a word cloud looking at the 'description' text\nfrom wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\n\n# Wordcloud functions input needs to be a single string of text\n# here I'm concatenating all descriptions into a single string\nreviews_str = reviews_sample.description.str.cat ()\nwordcloud = WordCloud (background_color = 'white').generate (reviews_str)\nplt.figure (figsize = (12, 12))\nplt.imshow (wordcloud, interpolation = 'bilinear')\nplt.axis ('off')\nplt.show ()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:49.911401Z","iopub.execute_input":"2022-06-25T13:14:49.911631Z","iopub.status.idle":"2022-06-25T13:14:50.794957Z","shell.execute_reply.started":"2022-06-25T13:14:49.911603Z","shell.execute_reply":"2022-06-25T13:14:50.794079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now let's split the data into negative (score 1 or 2) and positive (4 or 5) reviews\nnegative_reviews = reviews_sample [reviews_sample ['review.point'].isin ([83, 85])]\npositive_reviews = reviews_sample [reviews_sample ['review.point'].isin ([90, 93])]\n\n# transform to single string \nnegative_reviews_str = negative_reviews.description.str.cat ()\npositive_reviews_str = positive_reviews.description.str.cat ()\n\n# create wordclouds\nwordcloud_negative = WordCloud (background_color = 'white').generate (negative_reviews_str)\nwordcloud_positive = WordCloud (background_color = 'white').generate (positive_reviews_str)\n\n# Plot\nfig = plt.figure (figsize = (12, 12))\nax1 = fig.add_subplot (211)\nax1.imshow (wordcloud_negative, interpolation = 'bilinear')\nax1.axis ('off')\nax1.set_title ('Reviews with negative scores', fontsize = 20)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:50.79609Z","iopub.execute_input":"2022-06-25T13:14:50.796301Z","iopub.status.idle":"2022-06-25T13:14:51.810694Z","shell.execute_reply.started":"2022-06-25T13:14:50.796274Z","shell.execute_reply":"2022-06-25T13:14:51.810028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure (figsize = (12, 12))\nax2 = fig.add_subplot (212)\nax2.imshow (wordcloud_positive, interpolation = 'bilinear')\nax2.axis ('off')\nax2.set_title (\"Reviews with positive scores\", fontsize = 20)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:51.811783Z","iopub.execute_input":"2022-06-25T13:14:51.81218Z","iopub.status.idle":"2022-06-25T13:14:52.074851Z","shell.execute_reply.started":"2022-06-25T13:14:51.812127Z","shell.execute_reply":"2022-06-25T13:14:52.074109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Summarizing text data**","metadata":{}},{"cell_type":"code","source":"# import BeautifulSoup and urllib libraries to fetch data from Wikipedia\nfrom bs4 import BeautifulSoup\nfrom urllib.request import urlopen \n\n# function to get data from Wikipedia\ndef get_only_text (url):\n    page = urlopen (url)\n    soup = BeautifulSoup (page)\n    text = ' '.join (map (lambda p: p.text, soup.find_all ('p')))\n    print (text)\n    return soup.title.text, text\n\n# mention from Wikipedia url\nurl = \"https://en.wikipedia.org/wiki/Natural_language_processing\"\n\n# call the function created above\ntext = get_only_text (url)\n\n# count the number of letters\nlen (\"\".join (text))\n","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:52.076053Z","iopub.execute_input":"2022-06-25T13:14:52.076268Z","iopub.status.idle":"2022-06-25T13:14:52.666793Z","shell.execute_reply.started":"2022-06-25T13:14:52.076242Z","shell.execute_reply":"2022-06-25T13:14:52.665922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's see first 500 letters from the text\ntext [:500]","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:52.668195Z","iopub.execute_input":"2022-06-25T13:14:52.668662Z","iopub.status.idle":"2022-06-25T13:14:52.674922Z","shell.execute_reply.started":"2022-06-25T13:14:52.668616Z","shell.execute_reply":"2022-06-25T13:14:52.674056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Clustering documents**","metadata":{}},{"cell_type":"code","source":"# import libraries\nimport numpy as np\nimport pandas as pd\nimport nltk\nfrom nltk.stem.snowball import SnowballStemmer\nfrom bs4 import BeautifulSoup\nimport re\nimport os\nimport codecs\nfrom sklearn import feature_extraction\nimport mpld3\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom sklearn.manifold import MDS\n\n# Let's use the same comcastcomplaints dataset we use for classification\ndata = pd.read_csv ('../input/comcastcomplaints/comcast_consumeraffairs_complaints.csv')\n\n# selecting required columns and rows\ndata = data [['text']]\ndata = data [pd.notnull (data ['text'])]\n\n# let's do the clustering for just 300 rows. It's easier to interpret\ndata_sample = data.sample (300)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:52.676173Z","iopub.execute_input":"2022-06-25T13:14:52.67654Z","iopub.status.idle":"2022-06-25T13:14:52.943277Z","shell.execute_reply.started":"2022-06-25T13:14:52.676494Z","shell.execute_reply":"2022-06-25T13:14:52.94237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Preprocessing and TF-IDF feature engineering**","metadata":{}},{"cell_type":"code","source":"# remove unwanted symbol\ndata_sample ['text'] = data_sample ['text'].str.replace ('XXXX', \"\")\n\n# convert dataframe to list\ncomplaints = data_sample ['text'].tolist ()\n\n# create the rank of documents - I will use it later\nranks = []\nfor i in range (1, len (complaints) + 1):\n    ranks.append (i)\n    \n# stop words\nstopwords = nltk.corpus.stopwords.words ('english')\n\n# load 'stemmer'\nstemmer = SnowballStemmer ('english')\n\n# functions for sentence tokenizer, to remove tokens and raw # pucntuation\ndef tokenize_and_stem (text):\n    tokens = [word for sent in nltk.sent_tokenize (text) for word in nltk.word_tokenize (sent)]\n    filtered_tokens = []\n    for token in tokens:\n        if re.search ('[a-zA-Z]', token):\n            filtered_tokens.append (token)\n    stems = [stemmer.stem (t) for t in filtered_tokens]\n    return stems\n\ndef tokenize_only (text):\n    tokens =[word.lower () for sent in nltk.sent_tokenize (text) for word\n            in nltk.word_tokenize (sent)]\n\n    filtered_tokens = []\n    for token in tokens:\n        if re.search ('[a-zA-Z]', token):\n            filtered_tokens.append (token)\n    return filtered_tokens\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer (max_df = 0.8, max_features = 200000, min_df = 0.2, stop_words = 'english',\n                                   use_idf = True, tokenizer = tokenize_and_stem, ngram_range = (1, 3))\n# fit the vectiorizer to data\ntfidf_matrix = tfidf_vectorizer.fit_transform (complaints)\nterms = tfidf_vectorizer.get_feature_names ()\nprint (tfidf_matrix.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:52.949568Z","iopub.execute_input":"2022-06-25T13:14:52.94983Z","iopub.status.idle":"2022-06-25T13:14:55.684273Z","shell.execute_reply.started":"2022-06-25T13:14:52.949798Z","shell.execute_reply":"2022-06-25T13:14:55.68327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Clustering using K-means**","metadata":{}},{"cell_type":"code","source":"# import Kmeans\nfrom sklearn.cluster import KMeans\n\n# define numbers of clusters\nnum_clusters = 6\n\n# running clustreing algorithm\nkm = KMeans (n_clusters = num_clusters)\nkm.fit (tfidf_matrix)\n\n# final clusters\nclusters = km.labels_.tolist ()\ncomplaints_data = {'rank': ranks, 'complaints': complaints, 'cluster': clusters}\nframe = pd.DataFrame (complaints_data, index = [clusters], columns = ['rank', 'cluster'])\n\n# number of docs per cluster\nframe ['cluster'].value_counts ()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:55.685413Z","iopub.execute_input":"2022-06-25T13:14:55.685666Z","iopub.status.idle":"2022-06-25T13:14:56.676367Z","shell.execute_reply.started":"2022-06-25T13:14:55.685625Z","shell.execute_reply":"2022-06-25T13:14:56.675732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Identify cluster behavior**","metadata":{}},{"cell_type":"code","source":"totalvocab_stemmed = []\ntotalvocab_tokenized = []\nfor i in complaints:\n    allwords_stemmed = tokenize_and_stem (i)\n    totalvocab_stemmed.extend (allwords_stemmed)\n    allwords_tokenized = tokenize_only (i)\n    totalvocab_tokenized.extend (allwords_tokenized)\nvocab_frame = pd.DataFrame ({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n\n# sort cluster centers by proximity to centroid\norder_centroids = km.cluster_centers_.argsort ()[:, ::-1]\nfor i in range (num_clusters):\n    print (\"Cluster %d words:\" % i, end = \"\")\n    for ind in order_centroids [i, :6]:\n        print (' %s' % vocab_frame.loc [terms [ind].split (' ')].\n              values.tolist ()[0][0].encode ('utf-8', 'ignore'), end = ',')\n        print ()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:14:56.677568Z","iopub.execute_input":"2022-06-25T13:14:56.677793Z","iopub.status.idle":"2022-06-25T13:15:00.548138Z","shell.execute_reply.started":"2022-06-25T13:14:56.677764Z","shell.execute_reply":"2022-06-25T13:15:00.547288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plot the clusters on a 2D graph**","metadata":{}},{"cell_type":"code","source":"# similarity\nsimilarity_distance = 1 - cosine_similarity (tfidf_matrix)\n\n# convert two components as I'm plotting points in a two-dimensional plane\nmds = MDS (n_components = 2, dissimilarity = 'precomputed', random_state = 1)\npos = mds.fit_transform (similarity_distance)\nxs, ys = pos [:, 0], pos [:, 1]\n\n# set up colors per clusters using a dict\nclusters_colors = {0: '#1b9e77', 1: '#d95f020', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e', 5: '#S2691E'}\n\n# set up cluster names using a dict\ncluster_names = {0: 'boxes, installation, cable',\n                 1: 'did, said, month',\n                 2: 'phone, account, year',\n                 3: 'customer, cable, time',\n                 4: 'tech, days, problem',\n                 5: 'internet, month, only'}\n\n# finally plot it\n%matplotlib inline\n\n# create data frame that has the result of the MDS and the cluster\ndf = pd.DataFrame (dict (x = xs, y = ys, label = clusters))\ngroups = df.groupby ('label')\n\n# set up plot\nfig, ax = plt.subplots(figsize = (17, 9))\nfor name, group in groups:\n    ax.plot (group.x, group.y, marker = 'o', linestyle = \"\", ms = 20,\n            label = cluster_names, mec = 'none')\nax.set_aspect ('auto')\nax.tick_params (axis = 'x', which = 'both', bottom = 'off', top = 'off', labelbottom = 'off')\nax.tick_params (axis = 'y', which = 'both', left = 'off', top = 'off', labelleft = 'off')\nax.legend (numpoints = 1)\nplt.show ()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T13:15:00.549387Z","iopub.execute_input":"2022-06-25T13:15:00.550073Z","iopub.status.idle":"2022-06-25T13:15:03.695796Z","shell.execute_reply.started":"2022-06-25T13:15:00.549978Z","shell.execute_reply":"2022-06-25T13:15:03.694789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Deep learning in Natural language processing**","metadata":{}},{"cell_type":"markdown","source":"**Classifying text**","metadata":{}},{"cell_type":"code","source":"# read the dataset\nimport pandas as pd\ndata_s = pd.read_csv ('../input/email-spam-dataset/lingSpam.csv')\n\n# preprocessing the data\nfrom nltk.corpus import stopwords\nfrom nltk import *\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.stem import WordNetLemmatizer\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n# remove stop words\nstop = stopwords.words ('english')\ndata_s ['Body'] = data_s ['Body'].apply (lambda x: \" \".join (x for x in x.split () if x not in stop))\n\n# rename column names\ndata_s = data_s.rename (columns = {\"Body\": \"Email\", \"Label\": \"Target\"})\ndata_s.head (10)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:13:59.507437Z","iopub.execute_input":"2022-06-27T15:13:59.507808Z","iopub.status.idle":"2022-06-27T15:14:11.710616Z","shell.execute_reply.started":"2022-06-27T15:13:59.507716Z","shell.execute_reply":"2022-06-27T15:14:11.709978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# delete punctuations, convert text in lower case and delete the double space\ndata_s ['Email'] = data_s ['Email'].apply (lambda x: re.sub ('[!@#$:).:,?&]', \"\", x.lower ()))\ndata_s ['Email'] = data_s ['Email'].apply (lambda x: re.sub (' ', ' ', x))\ndata_s ['Email'].head (10)                                                             ","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:14:52.877017Z","iopub.execute_input":"2022-06-27T15:14:52.877355Z","iopub.status.idle":"2022-06-27T15:14:54.202076Z","shell.execute_reply.started":"2022-06-27T15:14:52.877307Z","shell.execute_reply":"2022-06-27T15:14:54.201105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import required libraries\nimport sys, os, re, csv, codecs\nimport numpy as np\nimport pandas as pd\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\nfrom keras.models import Model\nfrom keras.models import Sequential\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\nfrom keras.models import Sequential\n\n# separating text and target classes\nlist_sentences_rawdata = data_s ['Email'].fillna (\"_na_\").sort_values\nlist_classes = [\"Target\"]\ntarget = data_s [list_classes].values\nTo_Process = data_s [[\"Email\", \"Target\"]]\n\n# train and test split with 80:20 ratio\ntrain, test = train_test_split (To_Process, test_size = 0.2)\n\n# define the sequence lengths, max number of words and embedding dimensions\nMAX_SEQUENCE_LENGTH = 400\n\n# top 20000 frequently occuring words\nMAX_NB_WORDS = 30000\n\n# get the frequently occuring words\ntokenizer = Tokenizer (num_words = MAX_NB_WORDS)\ntokenizer.fit_on_texts (train.Email)\ntrain_sequences = tokenizer.texts_to_sequences (train.Email)\ntest_sequences = tokenizer.texts_to_sequences (test.Email)\n\n# dictionary containing words and their index\nword_index = tokenizer.word_index\n\nprint (\"Found %s unique tokens.\" %len (word_index))\n\n# get only the top frequent words on train\ntrain_data = pad_sequences (train_sequences, maxlen = MAX_SEQUENCE_LENGTH)\n\n# get only the top frequent words on test\ntest_data = pad_sequences (test_sequences, maxlen = MAX_SEQUENCE_LENGTH)\n\nprint (train_data.shape)\nprint (test_data.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:15:03.916482Z","iopub.execute_input":"2022-06-27T15:15:03.916807Z","iopub.status.idle":"2022-06-27T15:15:15.081285Z","shell.execute_reply.started":"2022-06-27T15:15:03.916769Z","shell.execute_reply":"2022-06-27T15:15:15.080257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels = train ['Target']\ntest_labels = test ['Target']\n\n# import library\nfrom sklearn.preprocessing import LabelEncoder\n\n# converts the character array to numeric array. Assigns levels to unique labels\nle = LabelEncoder ()\nle.fit (train_labels)\ntrain_labels = le.transform (train_labels)\ntest_labels = le.transform (test_labels)\n\nprint (le.classes_)\nprint (np.unique (train_labels, return_counts = True))\nprint (np.unique (test_labels, return_counts = True))","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:15:18.702829Z","iopub.execute_input":"2022-06-27T15:15:18.703154Z","iopub.status.idle":"2022-06-27T15:15:18.713258Z","shell.execute_reply.started":"2022-06-27T15:15:18.703122Z","shell.execute_reply":"2022-06-27T15:15:18.712677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# changing data types\nlabels_train = to_categorical (np.asarray (train_labels))\nlabels_test = to_categorical (np.asarray (test_labels))\nprint (\"Shape of data tensor:\", train_data.shape)\nprint (\"Shape of label tensor:\", labels_train.shape)\nprint (\"Shape of label tensor:\", labels_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:15:23.929479Z","iopub.execute_input":"2022-06-27T15:15:23.929947Z","iopub.status.idle":"2022-06-27T15:15:23.938723Z","shell.execute_reply.started":"2022-06-27T15:15:23.929901Z","shell.execute_reply":"2022-06-27T15:15:23.937897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EMBEDDING_DIM = 200\nprint (MAX_SEQUENCE_LENGTH)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:15:26.025827Z","iopub.execute_input":"2022-06-27T15:15:26.026475Z","iopub.status.idle":"2022-06-27T15:15:26.030785Z","shell.execute_reply.started":"2022-06-27T15:15:26.026437Z","shell.execute_reply":"2022-06-27T15:15:26.030177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model building and predicting**","metadata":{}},{"cell_type":"code","source":"print ('Training CNN 1D model.')\nmodel = Sequential ()\nmodel.add (Embedding (MAX_NB_WORDS, EMBEDDING_DIM, input_length = MAX_SEQUENCE_LENGTH))\nmodel.add (Dropout (0.5))\nmodel.add (Conv1D (128, 5, activation = 'relu'))\nmodel.add (MaxPooling1D (5))\nmodel.add (Dropout (0.5))\nmodel.add (BatchNormalization ())\nmodel.add (Conv1D (128, 5, activation = 'relu'))\nmodel.add (MaxPooling1D (5))\nmodel.add (Dropout (0.5))\nmodel.add (BatchNormalization ())\nmodel.add (Flatten ())\nmodel.add (Dense (128, activation = 'relu'))\nmodel.add (Dense (2, activation = 'softmax'))\nmodel.compile (loss = 'categorical_crossentropy', optimizer = 'rmsprop', metrics = ['acc'])\nmodel.fit (train_data, labels_train, batch_size = 64, epochs = 5, validation_data = (test_data, labels_test))","metadata":{"execution":{"iopub.status.busy":"2022-06-26T17:48:43.272775Z","iopub.execute_input":"2022-06-26T17:48:43.273603Z","iopub.status.idle":"2022-06-26T17:50:07.419176Z","shell.execute_reply.started":"2022-06-26T17:48:43.273549Z","shell.execute_reply":"2022-06-26T17:50:07.418281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predictions on test data\npredicted = model.predict (test_data)\npredicted","metadata":{"execution":{"iopub.status.busy":"2022-06-26T17:50:08.215501Z","iopub.execute_input":"2022-06-26T17:50:08.215954Z","iopub.status.idle":"2022-06-26T17:50:08.886883Z","shell.execute_reply.started":"2022-06-26T17:50:08.215919Z","shell.execute_reply":"2022-06-26T17:50:08.886078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model evaluation\nimport sklearn\nfrom sklearn.metrics import precision_recall_fscore_support as score\nprecision, recall, fscore, support = score (labels_test, predicted.round(), zero_division = 1)\nprint (\"Precision: {}\".format (precision))\nprint (\"Recall: {}\".format (recall))\nprint (\"Fscore: {}\".format (fscore))\nprint (\"Support: {}\".format (support))\nprint (\"----------------------------\")\nprint (sklearn.metrics.classification_report (labels_test, predicted.round (), zero_division = 1))","metadata":{"execution":{"iopub.status.busy":"2022-06-26T17:50:13.784062Z","iopub.execute_input":"2022-06-26T17:50:13.784359Z","iopub.status.idle":"2022-06-26T17:50:13.816714Z","shell.execute_reply.started":"2022-06-26T17:50:13.784325Z","shell.execute_reply":"2022-06-26T17:50:13.815931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define RNN model\nfrom keras.layers.recurrent import SimpleRNN\n\n# model training\nprint ('Training SIMPLERNN model.')\nmodel = Sequential ()\nmodel.add (Embedding (MAX_NB_WORDS, EMBEDDING_DIM, input_length = MAX_SEQUENCE_LENGTH))\nmodel.add (SimpleRNN (2, input_shape = (None, 1)))\nmodel.add (Dense (2, activation = 'softmax'))\nmodel.compile (loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\nmodel.fit (train_data, labels_train, batch_size = 16, epochs = 5, validation_data = (test_data, labels_test))","metadata":{"execution":{"iopub.status.busy":"2022-06-26T17:50:16.722322Z","iopub.execute_input":"2022-06-26T17:50:16.723139Z","iopub.status.idle":"2022-06-26T17:52:03.833781Z","shell.execute_reply.started":"2022-06-26T17:50:16.723097Z","shell.execute_reply":"2022-06-26T17:52:03.832657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prediction on test data\npredicted_Srnn = model.predict (test_data)\npredicted_Srnn","metadata":{"execution":{"iopub.status.busy":"2022-06-26T17:52:06.166109Z","iopub.execute_input":"2022-06-26T17:52:06.166399Z","iopub.status.idle":"2022-06-26T17:52:06.803738Z","shell.execute_reply.started":"2022-06-26T17:52:06.166365Z","shell.execute_reply":"2022-06-26T17:52:06.803076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model evaluation\nfrom sklearn.metrics import precision_recall_fscore_support as score\n\nprecision, recall, fscore, support = score (labels_test, predicted_Srnn.round ())\nprint ('precision: {}'.format (precision))\nprint ('recall: {}'.format (recall))\nprint ('fscor: {}'.format (fscore))\nprint ('support: {}'.format (support))\n\nprint ('----------------------------')\n\nprint (sklearn.metrics.classification_report (labels_test, predicted_Srnn.round ()))","metadata":{"execution":{"iopub.status.busy":"2022-06-26T17:56:52.177785Z","iopub.execute_input":"2022-06-26T17:56:52.178092Z","iopub.status.idle":"2022-06-26T17:56:52.205211Z","shell.execute_reply.started":"2022-06-26T17:56:52.178056Z","shell.execute_reply":"2022-06-26T17:56:52.204303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model training LSTM (Long short-term memory)\nprint ('Training LSTM model.')\nmodel = Sequential ()\nmodel.add (Embedding (MAX_NB_WORDS, EMBEDDING_DIM, input_length = MAX_SEQUENCE_LENGTH))\nmodel.add (LSTM (activation = 'relu', return_sequences = True, units = 128))\nmodel.add (Dropout (0.2))\nmodel.add (BatchNormalization ())\nmodel.add (Flatten ())\nmodel.add (Dense (2, activation = 'softmax'))\nmodel.compile (loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\nmodel.fit (train_data, labels_train, batch_size = 16, epochs = 5, validation_data = (test_data, labels_test))","metadata":{"execution":{"iopub.status.busy":"2022-06-26T18:24:49.261427Z","iopub.execute_input":"2022-06-26T18:24:49.261729Z","iopub.status.idle":"2022-06-26T18:31:13.430541Z","shell.execute_reply.started":"2022-06-26T18:24:49.261693Z","shell.execute_reply":"2022-06-26T18:31:13.429876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predicition on text data\npredicted_lstm = model.predict (test_data)\npredicted_lstm","metadata":{"execution":{"iopub.status.busy":"2022-06-26T18:31:19.518913Z","iopub.execute_input":"2022-06-26T18:31:19.519217Z","iopub.status.idle":"2022-06-26T18:31:22.229378Z","shell.execute_reply.started":"2022-06-26T18:31:19.519178Z","shell.execute_reply":"2022-06-26T18:31:22.228645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model evaluation\nfrom sklearn.metrics import precision_recall_fscore_support as score\nprecision, recall, fscore, support = score (labels_test, predicted_lstm.round ())\n\nprint ('precision: {}'.format (precision))\nprint ('recall: {}'.format (recall))\nprint ('fscore: {}'.format (fscore))\nprint ('support: {}'.format (support))\n\nprint ('----------------------------')\n\nprint (sklearn.metrics.classification_report (labels_test, predicted_lstm.round ()))","metadata":{"execution":{"iopub.status.busy":"2022-06-26T18:31:24.879921Z","iopub.execute_input":"2022-06-26T18:31:24.88023Z","iopub.status.idle":"2022-06-26T18:31:24.906622Z","shell.execute_reply.started":"2022-06-26T18:31:24.880181Z","shell.execute_reply":"2022-06-26T18:31:24.905817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# bidirectional LSTM \n# model training\nprint ('Training Bidirectional LSTM model.')\nmodel = Sequential ()\nmodel.add (Embedding (MAX_NB_WORDS, EMBEDDING_DIM, input_length = MAX_SEQUENCE_LENGTH))\nmodel.add (Bidirectional (LSTM (16, return_sequences = True, dropout = 0.1, recurrent_dropout = 0.1)))\nmodel.add (Conv1D (16, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\"))\nmodel.add (GlobalMaxPool1D ())\nmodel.add (Dense (50, activation = \"relu\"))\nmodel.add (Dropout (0.1))\nmodel.add (Dense (2, activation = \"softmax\"))\nmodel.compile (loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\nmodel.fit (train_data, labels_train, batch_size = 16, epochs = 3, validation_data = (test_data, labels_test))","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:15:58.020493Z","iopub.execute_input":"2022-06-27T15:15:58.020804Z","iopub.status.idle":"2022-06-27T15:24:27.457638Z","shell.execute_reply.started":"2022-06-27T15:15:58.020773Z","shell.execute_reply":"2022-06-27T15:24:27.456908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prediction on test data\npredicted_blstm = model.predict (test_data)\npredicted_blstm","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:24:29.595019Z","iopub.execute_input":"2022-06-27T15:24:29.595731Z","iopub.status.idle":"2022-06-27T15:24:35.827188Z","shell.execute_reply.started":"2022-06-27T15:24:29.59569Z","shell.execute_reply":"2022-06-27T15:24:35.826273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model evaluation\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.metrics import classification_report\nprecision, recall, fscore, support = score (labels_test, predicted_blstm.round ())\n\nprint ('precision: {}'.format (precision))\nprint ('recall: {}'.format (recall))\nprint ('fscore: {}'.format (fscore))\nprint ('support: {}'.format (support))\n\nprint ('----------------------------')\n\nprint (classification_report (labels_test, predicted_blstm.round ()))","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:30:04.008766Z","iopub.execute_input":"2022-06-27T15:30:04.009069Z","iopub.status.idle":"2022-06-27T15:30:04.038365Z","shell.execute_reply.started":"2022-06-27T15:30:04.009039Z","shell.execute_reply":"2022-06-27T15:30:04.036763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Next word prediction**","metadata":{}},{"cell_type":"code","source":"# import required libraries\nimport numpy as np\nimport random\nimport pandas as pd\nimport sys\nimport os\nimport time\nimport codecs\nimport collections\nimport numpy\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import LSTM\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.utils import np_utils\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport scipy\nfrom scipy import spatial\nfrom nltk.tokenize.toktok import ToktokTokenizer\nimport re\n\ncontent = pd.read_csv ('../input/email-spam-dataset/completeSpamAssassin.csv')\n\n# just selecting emails and converting it into list\nemail = content [['Body']]\nlist_data = email.values.tolist ()\n\ntokenizer = ToktokTokenizer ()","metadata":{"execution":{"iopub.status.busy":"2022-06-28T18:07:24.786923Z","iopub.execute_input":"2022-06-28T18:07:24.787219Z","iopub.status.idle":"2022-06-28T18:07:24.907925Z","shell.execute_reply.started":"2022-06-28T18:07:24.787188Z","shell.execute_reply":"2022-06-28T18:07:24.906492Z"},"trusted":true},"execution_count":5,"outputs":[]}]}